# Critique

In the article "Multi-Armed Recommender System Bandit Ensembles "CaÃ±amares, R., Redondo, M., & Castells, P. (2019)" a novel ensemble method is presented yielding better results than traditional ensemble methods with lower computation costs.

The authors implemented an multi-armed bandit approach in which the different recommender systems to be ensembled are represented as arms and the ensemble itself is the bandit that chooses what arm to use in the next round of reccomendations. Two basic bandit algorithms were used on the experiments: e-greedy and Thompson.

The context of the bandit is the target user, the arms are the recommendation algorithms, the reward is 1 if the user is pleased with the recommendation and 0 otherwise and the arms parameters can be updated after through the passing of rounds. The dataset used is the MovieLens 1M dataset with 6K users and 3.7K movies. The ratings were binarized by mapping values 1-3 to 0 and 4-5 to 1. The arms of the bandit approach were user-kNN with cosine similarity and all users as neighbors, matrix factorization with learning rate 0.1, 20 latent factors and 20 iterations, and most-popular recommendations. In each round, one item is being recommended so arms are being compared by their Precision@1 scores, selecting the one with the highest value in every round.

The initial training set included 5% of the ratings sampled uniformly at random. The training set grew in each round as in every prediction, the corresponding datapoint in the testing set was added to the training set at the end of the round.

200 epochs of the ensemble method were executed. Results were presented by comparing each experiment's cumulative recall through the epochs. The relevant findings of the experiments are that both bandit ensemble methods were superior than every single algorithm and to the traditional dynamic ensemble. The dynamic ensemble yielded the same cumulative recall of the matrix factorization algorithm due to a feedback loop effect in which the ensemble tends to exploit the algorithm selected in the first iteration keeping it thereafter by it.

The behavior of the bandit method is worth noting and it explains why it yields better results than most popular and the collaborative filtering methods on their own. With both e-greedy and Thompson sampling bandits, at the first epochs, the algorithm selected to recommend was Most Popular. As the CF algorithms got more training data and more epochs to update their parameters, they were beginning to be selected by the bandit. This approach is great because it diminishes the negative effects of the cold start problem of CF models as Most Popular is being used meanwhile the CF algorithms are training until they are ready to provide better results than Most Popular. Moreover the following is stated on the article:

  > We also see that even though our two bandits soon favor matrix factorization as a preferred algorithm, the initial reliance on popularity, albeit short, seems to not only directly improve the ensemble, but also considerably (indirectly) enhance the behavior of matrix factorization itself thereafter (by collecting a richer pool of user feedback as input data), compared to an exclusive and persistent reliance on matrix factorization alone, as does the dynamic ensemble. The continued (albeit small) fraction of alternation among algorithms, can also help in the same fashion.

The fact of Most Popular taking over the first round of reccomendations means an opportunity of better improvement for the matrix factorization algorithm. This is contrasted with the dynamic ensemble which does not allow room for a richer pool retrieval by the MF algorithm. Also a great benefit from this method is that only one algorithm runs at a time opposed to other ensembles in which multiple algorithms run at the same time.

The article is straightforward and it keeps the way 'clean' for the bandit method analysis. This is shown in how the authors did not get over complicated with the selection of the algorithms used as arms on the experiments, nor on their settings and hiperparameters. This helps to keep the focus on the main topic on the article as the narrative is not deviated towards the algorithm's details. The only downside to their choice for simplicity is that only two bandit methods were analyzed being an article which its main object of study is precisely the bandit method. In [1], even though the research focused on Carousel personalization with the usage of Contextual bandits, their experiments considered 6 different methods. The article could have benefited by considering more methods as it proves with more confidence that multi-armed bandit as a general strategy is a good dynamic recommender ensemble.

## References

[1] Bendada, W., Salha, G., & Bontempelli, T. (2020). Carousel Personalization in Music Streaming Apps with Contextual Bandits. In Fourteenth ACM Conference on Recommender Systems (pp. 420-425).