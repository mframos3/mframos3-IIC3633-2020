# Critique

The following document is a continuation of the previous [critique](lectura10-1.md) of the article "Deep learning based recommender system: A survey and new perspectives. Zhang, S., Yao, L., Sun, A., & Tay, Y. (2019).".

The article on section 3.5 up to section 4 continues to point out several deep learning models in a way that I personally dislike. The presentation of the models is brief, reliant on references in order to be understood and it addresses the technicality with all its complexity without a building up on it. It is supposed to be an article to get the readers into the topic of deep learning on recommender systems, but at the same time it expects the reader to know about these topics beforehand. It is oriented more for experienced people on deep learning that want to know about its applications on RecSys than the way round (experience on RecSys that want to know about deep learning models for recommendation).

My highlights of sections 3.5 up to 4 are:
  
- **RNN model optimizations**: Overall, the models presented are being constructed and modified in creative ways to fulfill diverse purposes. On the previous critique I mentioned the ensemble of traditional RecSys models with DL models as a creative way to incorporate the perks of both approaches into a better model. On section 3.5 regarding RNNs, an interesting technique to reduce computation costs is to collapse older states into a single history state. It makes sense due to the fact that in a sequential model, generally older states are less important than most recent states so it is acceptable to simplify the representation of older states in a single history state in order to reduce computational costs.
- **Multi task recommendation**: I believe that the paradigm of working with multi task recommendations may be able to deviate the attention from the obsession for high acurracy scores as the are multiple objectives on the models to address. Thinking in multi-tasking may focus more on quality of recommendations as it should be. For instance the main task can be rating prediction but some other features can be added to the recommendation by having as auxiliary tasks the prediction/suggestion of tags, text or images.This can enchance and complement the main rating prediction with extra content. 
- **Attention based systems**: Since its first appearance on section 3.7, Attention as a topic is addressed until the end of the article covering aspects that go beyond models presentations such as its impact on DL explainability. Attention is used to address tasks on its own or as an aid to MLP, RNNs and CNNs models on the main tasks by enchancing recommendation performance.

In section 4, Future research direction and open issues is presented. On this section the autors state the following regarding the benefits of DL as a tool to reduce the work put into feature engineering:

> Additionally, feature engineering has not been fully studied in the recommendation research community, but it is essential and widely employed in industrial applications [20, 27]. However, most of the existing models require manually crafted and selected features, which is time-consuming and tedious. Deep neural network is a promising tool for automatic feature crafting by reducing manual intervention [129]. There is also an added advantage of representation learning from free texts, images or data that exists in the ‘wild’ without having to design intricate feature engineering pipelines. More intensive studies on deep feature engineering specific for recommender systems are expected to save human efforts as well as improve recommendation quality.

It is true that DL models are able to build representations of the input without ,much feature engineering and extensive pre-processing but it does not consider the added difficulty on building the models and its parameter tuning. In order to affirm that DL makes 'life easier', the comparison between the work needed on feature engineering and the work needed to build the model with its tuning should be made. Once that comparison is presented, then it is safe to suggest that DL reduces manual intervention. Though it is not explicitly stated, the authors decided to include this aspect of DL models as something interesting to point out.

Lastly, the authors addressed the lack of rigorous evaluations of the models. With the question: *Why is there no MNIST, ImageNet or SQuAD for recommender systems?* there is some sort of demand for a standardized evaluation guidelines for any recommender systems research involving datasets, splits and baselines for benchmarking.
